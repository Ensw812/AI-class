# ================================
# Logistic Regression (from scratch) — Completed
# ================================

# 0) Imports
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.preprocessing import StandardScaler

# 1) Load / prepare data
# --------------------------------------------------
# Assumptions:
#   - Binary classification with labels in {0,1}
#   - X is an (m, n) feature matrix (no bias column yet)
#   - y is an (m,) label vector

def load_data():
    # Load dataset
    df = pd.read_csv("dataset 2.csv")
    df["created_at"] = pd.to_datetime(df["created_at"])
    df["actual_delivery_time"] = pd.to_datetime(df["actual_delivery_time"])
    df["delivery_duration_min"] = (df["actual_delivery_time"] - df["created_at"]).dt.total_seconds() / 60.0

    # Convert to binary classification (0 = Fast, 1 = Slow)
    def categorize_time_binary(x):
        if x < 30:
            return 0  # Fast delivery
        else:
            return 1  # Slow delivery

    df["delivery_binary"] = df["delivery_duration_min"].apply(categorize_time_binary)

    # Use one feature: total_items
    X = df[["total_items"]].values  # shape: (m, 1)
    y = df["delivery_binary"].values  # shape: (m,)

    # Feature scaling (standardization)
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # No train-test split specified in skeleton, so use full dataset
    return X_scaled, y, scaler

# Load data
X, y, scaler = load_data()
m = X.shape[0]  # number of samples

# Add bias column (x0 = 1)
X = np.column_stack([np.ones(m), X])  # shape: (m, n+1)
n_with_bias = X.shape[1]  # n+1 (1 feature + bias)

# 2) Utility functions: sigmoid, loss, gradient, prediction
# --------------------------------------------------
def sigmoid(z):
    # σ(z) = 1 / (1 + e^-z)
    # Clip z to avoid overflow in exp
    z = np.clip(z, -500, 500)
    return 1 / (1 + np.exp(-z))

def predict_proba(X, w):
    # p = σ(Xw)
    return sigmoid(np.dot(X, w))

def binary_cross_entropy(y_true, y_prob, eps=1e-12):
    # Log-loss = - [ y log(p) + (1-y) log(1-p) ]
    # Clip probabilities to avoid log(0)
    y_prob = np.clip(y_prob, eps, 1 - eps)
    return -np.mean(y_true * np.log(y_prob) + (1 - y_true) * np.log(1 - y_prob))

def gradient(X, y_true, y_prob):
    # ∂J/∂w = (1/m) X^T (p - y)
    m = X.shape[0]
    return (1/m) * np.dot(X.T, (y_prob - y_true))

# 3) Initialize parameters
# --------------------------------------------------
# Initialize weights to zeros (including bias)
w = np.zeros(n_with_bias)  # shape: (n+1,) = (2,) for 1 feature + bias

# 4) Hyperparameters
# --------------------------------------------------
learning_rate = 0.01  # Reasonable for standardized features
num_iterations = 1000  # Sufficient for convergence in simple cases

# For tracking
cost_history = []

# 5) Gradient Descent loop
# --------------------------------------------------
for i in range(num_iterations):
    # Forward pass: compute probabilities
    y_prob = predict_proba(X, w)
    
    # Compute loss (log-loss / cross-entropy)
    cost = binary_cross_entropy(y, y_prob)
    cost_history.append(cost)
    
    # Backward pass: compute gradient
    grad = gradient(X, y, y_prob)
    
    # Parameter update
    w -= learning_rate * grad

# 6) Final parameters
# --------------------------------------------------
print("Final parameters (w):")
print(w)  # w[0] is bias term

# 7) Plot: Cost vs Iterations
# --------------------------------------------------
plt.figure()
plt.plot(range(len(cost_history)), cost_history)
plt.xlabel("Iteration")
plt.ylabel("Cost (Log-Loss)")
plt.title("Training: Cost vs. Iterations")
plt.grid(True)
plt.show()

# 8) Plot: Cost vs 3 of the most important parameters
# --------------------------------------------------
# Since we only have 1 feature + bias, we'll plot for both parameters (w[0] = bias, w[1] = total_items)
param_indices = [0, 1]  # Include bias and the single feature
print("Plotting cost sensitivity for parameter indices:", param_indices)

def compute_cost_given_w(mod_w):
    # Helper to compute cost for a modified parameter vector
    y_hat_mod = predict_proba(X, mod_w)
    return binary_cross_entropy(y, y_hat_mod)

# For each chosen parameter, sweep values and compute cost
for idx in param_indices:
    center = w[idx]
    # Sweep range: ±1.0 around the trained value
    sweep = np.linspace(center - 1.0, center + 1.0, 60)
    
    costs = []
    for val in sweep:
        w_tmp = w.copy()
        w_tmp[idx] = val
        costs.append(compute_cost_given_w(w_tmp))
    
    plt.figure()
    plt.plot(sweep, costs)
    plt.xlabel(f"Parameter w[{idx}]" + (" (bias)" if idx == 0 else " (total_items)"))
    plt.ylabel("Cost (Log-Loss)")
    plt.title(f"Cost vs Parameter w[{idx}] (holding others fixed)")
    plt.grid(True)
    plt.show()

# 9) (Optional) Inference helper
# --------------------------------------------------
def predict_label(X_new, w, threshold=0.5):
    # Returns 0/1 predictions based on threshold
    return (predict_proba(X_new, w) >= threshold).astype(int)

# Compute training accuracy
preds = predict_label(X, w)
accuracy = (preds == y).mean()
print("Training accuracy (for reference only):", accuracy)

# 10) Plot the full S-shaped logistic regression curve
# --------------------------------------------------
# To match your sklearn-based code, plot the logistic curve
x_vals = np.linspace(-10, 40, 500).reshape(-1, 1)  # Raw feature range
x_vals_scaled = scaler.transform(x_vals)  # Scale the feature
x_vals_with_bias = np.column_stack([np.ones(x_vals.shape[0]), x_vals_scaled])  # Add bias term
y_prob = predict_proba(x_vals_with_bias, w)  # Get probabilities

plt.figure(figsize=(8,6))
plt.scatter(df["total_items"], y, c=y, cmap="bwr", alpha=0.6, label="Actual Data (0=Fast,1=Slow)")
plt.plot(x_vals, y_prob, 'g-', linewidth=2, label="Logistic Regression Curve (S-shape)")
plt.title("Logistic Regression Fit (Full S-curve)")
plt.xlabel("total_items")
plt.ylabel("P(Slow Delivery)")
plt.legend()
plt.grid(True)
plt.show()
