# 0) Imports
import os, glob
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.preprocessing import StandardScaler
import kagglehub  

# 1) Load / prepare data

def load_data():
    """
    Downloads the Kaggle dataset via kagglehub and loads a CSV.
    Returns:
      X_scaled : (m, 1) standardized feature matrix for 'total_items'
      y        : (m,) binary labels (0 fast, 1 slow)
      scaler   : fitted StandardScaler
      df       : original dataframe (for plotting)
    """
    # Download latest version of the dataset folder
    path = kagglehub.dataset_download("ranitsarkar01/porter-delivery-time-estimation")
    print("Path to dataset files:", path)

    preferred = os.path.join(path, "dataset 2.csv")
    if os.path.exists(preferred):
        csv_path = preferred
    else:
        candidates = sorted(glob.glob(os.path.join(path, "*.csv")))
        if not candidates:
            raise FileNotFoundError("No CSV file found in the Kaggle dataset folder.")
        csv_path = candidates[0]
        print(f"Using CSV file: {os.path.basename(csv_path)}")

     Load dataset
    df = pd.read_csv(csv_path)

    # Parse timestamps and compute delivery duration (minutes)
    df["created_at"] = pd.to_datetime(df["created_at"])
    df["actual_delivery_time"] = pd.to_datetime(df["actual_delivery_time"])
    df["delivery_duration_min"] = (
        df["actual_delivery_time"] - df["created_at"]
    ).dt.total_seconds() / 60.0

    # Binary target (0 = Fast, 1 = Slow)
    df["delivery_binary"] = (df["delivery_duration_min"] >= 30).astype(int)

    # Feature: total_items
    X = df[["total_items"]].values  # shape: (m, 1)
    y = df["delivery_binary"].values  # shape: (m,)

    # Standardize the feature
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    return X_scaled, y, scaler, df

# Load data
X_scaled, y, scaler, df = load_data()
m = X_scaled.shape[0]

# Add bias column (x0 = 1)
X = np.column_stack([np.ones(m), X_scaled])  # shape: (m, 2) for bias + 1 feature
n_with_bias = X.shape[1]

# 2) Utility functions
def sigmoid(z):
    z = np.clip(z, -500, 500)
    return 1 / (1 + np.exp(-z))

def predict_proba(X, w):
    return sigmoid(np.dot(X, w))

def binary_cross_entropy(y_true, y_prob, eps=1e-12):
    y_prob = np.clip(y_prob, eps, 1 - eps)
    return -np.mean(y_true * np.log(y_prob) + (1 - y_true) * np.log(1 - y_prob))

def gradient(X, y_true, y_prob):
    m = X.shape[0]
    return (1/m) * np.dot(X.T, (y_prob - y_true))

# 3) Initialize parameters
w = np.zeros(n_with_bias)  # [bias, weight_total_items]

# 4) Hyperparameters
learning_rate = 0.01
num_iterations = 1000
cost_history = []

# 5) Gradient Descent
for i in range(num_iterations):
    y_prob = predict_proba(X, w)
    cost = binary_cross_entropy(y, y_prob)
    cost_history.append(cost)
    grad = gradient(X, y, y_prob)
    w -= learning_rate * grad

# 6) Final parameters
print("Final parameters (w):")
print(w)  # w[0] is bias

# 7) Cost vs Iterations
plt.figure()
plt.plot(range(len(cost_history)), cost_history)
plt.xlabel("Iteration")
plt.ylabel("Cost (Log-Loss)")
plt.title("Training: Cost vs. Iterations")
plt.grid(True)
plt.show()

# 8) Cost vs Parameters (bias and total_items weight)
param_indices = [0, 1]
print("Plotting cost sensitivity for parameter indices:", param_indices)

def compute_cost_given_w(mod_w):
    return binary_cross_entropy(y, predict_proba(X, mod_w))

for idx in param_indices:
    center = w[idx]
    sweep = np.linspace(center - 1.0, center + 1.0, 60)
    costs = []
    for val in sweep:
        w_tmp = w.copy()
        w_tmp[idx] = val
        costs.append(compute_cost_given_w(w_tmp))
    plt.figure()
    plt.plot(sweep, costs)
    plt.xlabel(f"Parameter w[{idx}]" + (" (bias)" if idx == 0 else " (total_items)"))
    plt.ylabel("Cost (Log-Loss)")
    plt.title(f"Cost vs Parameter w[{idx}] (holding others fixed)")
    plt.grid(True)
    plt.show()

# 9) Inference helper + training accuracy
def predict_label(X_new, w, threshold=0.5):
    return (predict_proba(X_new, w) >= threshold).astype(int)

preds = predict_label(X, w)
accuracy = (preds == y).mean()
print("Training accuracy (for reference only):", accuracy)

# 10) S-shaped logistic regression curve (in original units)
x_vals = np.linspace(
    max(0, df["total_items"].min() - 5),
    df["total_items"].max() + 5,
    500
).reshape(-1, 1)
x_vals_scaled = scaler.transform(x_vals)
x_vals_with_bias = np.column_stack([np.ones(x_vals.shape[0]), x_vals_scaled])
y_prob_curve = predict_proba(x_vals_with_bias, w)

plt.figure(figsize=(8, 6))
plt.scatter(df["total_items"].values, y, c=y, cmap="bwr", alpha=0.6,
            label="Actual Data (0=Fast,1=Slow)")
plt.plot(x_vals, y_prob_curve, linewidth=2, label="Logistic Regression Curve (S-shape)")
plt.title("Logistic Regression Fit (Full S-curve)")
plt.xlabel("total_items")
plt.ylabel("P(Slow Delivery)")
plt.legend()
plt.grid(True)
plt.show()
